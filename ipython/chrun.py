# -*- coding: utf-8 -*-
"""chrun.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C7N_fWabjOSl8q4WaxFpW4cbmVVhYHM1
"""

import warnings 
warnings.filterwarnings("ignore")
from pandas_profiling import ProfileReport
# Base libraries
import os
import numpy as np
import pandas as pd
import re
import string
import math
from IPython.display import display_html
import tqdm
#import wandb


## visualization libraries
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as patches
import seaborn as sns
#y!pip install pywaffle


#%matplotlib inline
sns.set(style="darkgrid")
pd.set_option('display.float_format', lambda x: '%.2f' % x)

import warnings 
warnings.filterwarnings("ignore")

# Base libraries
import os
import numpy as np
import pandas as pd
import re
import string
import math
from IPython.display import display_html
import tqdm
#import wandb


## visualization libraries
import matplotlib.pyplot as plt
import matplotlib as mpl
import matplotlib.patches as patches
import seaborn as sns
!pip install pywaffle
from pywaffle import Waffle

#%matplotlib inline
sns.set(style="darkgrid")
pd.set_option('display.float_format', lambda x: '%.2f' % x)
#devloping the model sklern library 
warnings.filterwarnings('ignore')
import os
from tqdm import tqdm_notebook
from pandas_profiling import ProfileReport
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2, f_classif
from sklearn.model_selection import KFold
from sklearn.feature_selection import SelectFromModel
from sklearn.svm import LinearSVC
from sklearn.impute import SimpleImputer
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import StandardScaler,OneHotEncoder,LabelEncoder,MinMaxScaler
from sklearn.compose import make_column_selector
# Model Building and Evaluation modules
from sklearn.model_selection import cross_validate,GridSearchCV
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import cross_val_predict

from sklearn.preprocessing import normalize
from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder,OrdinalEncoder
from sklearn.model_selection import train_test_split

from sklearn.pipeline import Pipeline, make_pipeline
from sklearn.decomposition import PCA

from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB
from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import CategoricalNB
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier

from sklearn.metrics import confusion_matrix, accuracy_score
from sklearn.metrics import precision_score, recall_score, f1_score
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV, cross_val_score
from sklearn.model_selection import GridSearchCV

from sklearn.naive_bayes import GaussianNB
from sklearn.linear_model import SGDClassifier, LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier, XGBRFClassifier
from xgboost import plot_tree, plot_importance

from sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE

from sklearn.decomposition import PCA
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

data=pd.read_csv('/WA_Fn-UseC_-Telco-Customer-Churn.csv')

class Customer_chrun():

    def __init__(self,data):
        self.data = data
        
    
    #EDA THE DATA OR EXPLORING THE DATA 

    def eda(self,data):
    
        print("----------Top-5 Records----------")
        print(data.head(5))
        print("-----------Information-----------")
      
        print(data.info())

        print("----------Details About The Data-----------")

        print(data.describe().T)
        print("-----------Data Types-----------")
        print(data.dtypes)
        print("----------Missing value-----------")
        print(data.isnull().sum())
        print("----------Null value-----------")
        print(data.isna().sum())
        print("----------Shape of Data----------")
        print(data.shape)

    def data_converting(self,data):

        data.TotalCharges = pd.to_numeric(data.TotalCharges, errors='coerce')
        print(" Succesfully Converted From Object to Numeric Value")
        
        print( data.isnull().sum())
        #Removing missing values 

        data.dropna(inplace = True)
        #remove the customer id 
        data=data.iloc[:,1:]
        data.head()
        
    def data_change(self,data):

        #Convertin the predictor variable in a binary numeric variable

        data['Churn'].replace(to_replace='Yes', value=1, inplace=True)
        data['Churn'].replace(to_replace='No',  value=0, inplace=True)

        #Let's convert all the categorical variables into dummy variables

        data = pd.get_dummies(data)
        
        print(data.head())
        
    def Normalizing_modelbuiding(self, data):
        ct =make_column_transformer((MinMaxScaler(),make_column_selector(dtype_include= np.number)),
                                     (OrdinalEncoder(),make_column_selector(dtype_include=object)))
                                     
        print(ct)

        y=data['Churn'].values
        X=data.drop(columns=['Churn'])
        X=ct.fit_transform(X)
        #spliting the data into 

        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=101) 
        X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)

        print(f'Total # of sample in whole dataset: {len(X)}')
        print(f'Total # of sample in train dataset: {len(X_train)}')
        print(f'Total # of sample in validation dataset: {len(X_valid)}')
        print(f'Total # of sample in test dataset: {len(X_test)}')


        models = {
        'GaussianNB': GaussianNB(),'MultinomialNB': MultinomialNB(),'BernoulliNB': BernoulliNB(),
        'LogisticRegression': LogisticRegression(),'RandomForestClassifier': RandomForestClassifier(),
        'SupportVectorMachine': SVC(),
        'DecisionTreeClassifier': DecisionTreeClassifier(),'KNeighborsClassifier': KNeighborsClassifier(),
        'GradientBoostingClassifier': GradientBoostingClassifier(),'Stochastic Gradient Descent':  SGDClassifier(max_iter=5000, random_state=0),'XGBClassifier': XGBClassifier()
        }

        modelNames = ["GaussianNB","MultinomialNB",'BernoulliNB','LogisticRegression','RandomForestClassifier',
                      'SupportVectorMachine','DecisionTreeClassifier', 'KNeighborsClassifier','GradientBoostingClassifier',
                      'Stochastic Gradient Descent', 'XGBClassifier']

        trainScores = []
        validationScores = []
        testScores = []

        for m in tqdm_notebook(models):


           model = models[m]
           model.fit(X_train, y_train)
           score = model.score(X_valid, y_valid)
           #print(f'{m} validation score => {score*100}')
    
           print(f'{m}') 
           train_score = model.score(X_train, y_train)
           print(f'Train score of trained model: {train_score*100}')
           trainScores.append(train_score*100)

           validation_score = model.score(X_valid, y_valid)
           print(f'Validation score of trained model: {validation_score*100}')
           validationScores.append(validation_score*100)

           test_score = model.score(X_test, y_test)
           print(f'Test score of trained model: {test_score*100}')
           testScores.append(test_score*100)
           print(" ")
    
           y_predictions = model.predict(X_test)
           conf_matrix = confusion_matrix(y_predictions, y_test)

           print(f'Confussion Matrix: \n{conf_matrix}\n')

           predictions = model.predict(X_test)
           cm = confusion_matrix(predictions, y_test)

           tn = conf_matrix[0,0]
           fp = conf_matrix[0,1]
           tp = conf_matrix[1,1]
           fn = conf_matrix[1,0]
           accuracy  = (tp + tn) / (tp + fp + tn + fn)
           precision = tp / (tp + fp)
           recall    = tp / (tp + fn)
           f1score  = 2 * precision * recall / (precision + recall)
           specificity = tn / (tn + fp)

           print(f'Accuracy : {accuracy}')
           print(f'Precision: {precision}')
           print(f'Recall   : {recall}')
           print(f'F1 score : {f1score}')
           print(f'Specificity : {specificity}')
           print("") 
           print(f'Classification Report: \n{classification_report(predictions, y_test)}\n')
           print("")
   
           for m in range (1):

            current = modelNames[m]
            modelNames.remove(modelNames[m])

            preds = model.predict(X_test)
            confusion_matr = confusion_matrix(y_test, preds) 
            #normalize = 'true'
            plt.figure(figsize = (16,10))
            plt.title(f'Customer_chrun   -   Model: {current}   -   Accuracy: {test_score*100}')
            sns.heatmap(confusion_matr, cmap="Blues", annot=True, annot_kws={"size": 16},
              xticklabels = ['Target - 0', 'Target - 1'],
              yticklabels=['Target - 0', 'Target - 1']);
            #plt.savefig(f'{current}.jpg')
            print("############################################################################")
            print("")
            print("") 
            print("")

    def vision(self,data):
        sns.set_style(style='darkgrid')
        sns.countplot(x='Contract',data=data)
        plt.title("Cancel count Details")
        plt.show()

    def pair(self,data):
        plt.figure(figsize=(12,8)) 
        sns.heatmap(data.corr(), annot=True, cmap='Dark2_r', linewidths = 2)
        plt.show()
        sns.pairplot(data)
        sns.set_style(style="darkgrid")
        plt.show()

flg=Customer_chrun(data=data)
print(flg)
flg.eda(data)
flg.vision(data)
flg.pair(data)
flg.data_converting(data)
flg.data_change(data)
flg.Normalizing_modelbuiding(data)